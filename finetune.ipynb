{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPaXwgfqB4nk",
    "outputId": "c4ec2e0c-1f53-4877-aa43-245c95ed939f"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch datasets tensorboard soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"tiny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8EENpg3CCes"
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperModel, WhisperTokenizer\n",
    "\n",
    "model_train = WhisperModel.from_pretrained(f\"openai/whisper-{MODEL}\").cuda().train()\n",
    "model_base = WhisperModel.from_pretrained(f\"openai/whisper-{MODEL}\").cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR-gqZaMjcme",
    "outputId": "5c87faf1-113d-4e2d-f388-35d5e2381c4d"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "ds = load_dataset(\"google/fleurs\", \"en_us\", split=\"train\")\n",
    "processor = WhisperProcessor.from_pretrained(f\"openai/whisper-{MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GdQTbc9bjqsB",
    "outputId": "b5a938ea-7e6c-4b9b-f4d1-079db38cb68a"
   },
   "outputs": [],
   "source": [
    "def get_sample(example):\n",
    "  waveform = example[\"audio\"][\"array\"]\n",
    "  sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "  # Use the model and processor to transcribe the audio:\n",
    "  input_features = processor(\n",
    "      waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    "  ).input_features\n",
    "\n",
    "  return {\n",
    "      \"length\": len(waveform) / sampling_rate,\n",
    "      \"input_features\": input_features,\n",
    "      \"input_ids\": processor.tokenizer.encode(example[\"raw_transcription\"].lower())\n",
    "  }\n",
    "\n",
    "if not( \".en\" in MODEL):\n",
    "    print(processor.get_decoder_prompt_ids(language=\"english\",task=\"transcribe\"))\n",
    "\n",
    "[processor.tokenizer.decode(i) for i in get_sample(ds[1])[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3llGbLgCKfO",
    "outputId": "cec01e6f-5859-4cbb-b23d-5b41a75bdcab"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def compute_partially_encoder(model, data, n_audio_ctx):\n",
    "  diffy = 2*n_audio_ctx - data.shape[2]\n",
    "\n",
    "  if diffy > 0:\n",
    "    data = nn.functional.pad(data, [0, diffy, 0, 0, 0, 0], \"constant\", 0.0)\n",
    "  elif diffy < 0:\n",
    "    data = data[:,:,:diffy]\n",
    "\n",
    "  if n_audio_ctx == 1500:\n",
    "    return model.encoder(data).last_hidden_state\n",
    "\n",
    "  input_embeds = nn.functional.gelu(model.encoder.conv1(data))\n",
    "  input_embeds = nn.functional.gelu(model.encoder.conv2(input_embeds))\n",
    "  input_embeds = input_embeds.permute(0, 2, 1)\n",
    "\n",
    "  embed_pos = model.encoder.embed_positions.weight[:n_audio_ctx]\n",
    "\n",
    "  hidden_states = input_embeds + embed_pos\n",
    "  hidden_states = nn.functional.dropout(hidden_states, p=model.encoder.dropout, training=model.encoder.training)\n",
    "\n",
    "  for idx, encoder_layer in enumerate(model.encoder.layers):\n",
    "    to_drop = False\n",
    "    if model.encoder.training:\n",
    "      dropout_probability = torch.rand([])\n",
    "      if dropout_probability < model.encoder.layerdrop:\n",
    "        to_drop = True\n",
    "\n",
    "    if to_drop:\n",
    "        layer_outputs = (None, None)\n",
    "    else:\n",
    "        if model.encoder.gradient_checkpointing and model.encoder.training:\n",
    "            layer_outputs = model.encoder._gradient_checkpointing_func(\n",
    "                encoder_layer.__call__,\n",
    "                hidden_states,\n",
    "                None,\n",
    "                None,\n",
    "                False,\n",
    "            )\n",
    "        else:\n",
    "            layer_outputs = encoder_layer(\n",
    "                hidden_states,\n",
    "                None,\n",
    "                layer_head_mask=None,\n",
    "                output_attentions=False,\n",
    "            )\n",
    "\n",
    "        hidden_states = layer_outputs[0]\n",
    "\n",
    "  hidden_states = model.encoder.layer_norm(hidden_states)\n",
    "  return hidden_states\n",
    "\n",
    "\n",
    "def compute_hidden_state_loss(model_train, model_base, optimizer, criterion, example):\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  n_ctx = int(round((1500.0 / 30.0) * example[\"length\"] ))\n",
    "\n",
    "  extra_ctx = torch.randint(-min(64, n_ctx // 3), min(64, n_ctx // 3), (1,)).item()\n",
    "  n_ctx += extra_ctx\n",
    "\n",
    "  input_features = example[\"input_features\"].cuda()\n",
    "  input_ids = torch.tensor([example[\"input_ids\"]], dtype=torch.long).cuda()\n",
    "\n",
    "  encoder_hidden_states_partial = compute_partially_encoder(model_train, input_features, n_ctx)\n",
    "  output_partial = model_train.decoder(\n",
    "      input_ids=input_ids,\n",
    "      encoder_hidden_states=encoder_hidden_states_partial,\n",
    "      output_hidden_states=True\n",
    "  )\n",
    "\n",
    "  with torch.no_grad():\n",
    "    encoder_hidden_states_full = compute_partially_encoder(model_base, input_features, 1500)\n",
    "    output_full = model_base.decoder(\n",
    "        input_ids=input_ids,\n",
    "        encoder_hidden_states=encoder_hidden_states_full,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "\n",
    "  loss = criterion(\n",
    "      #output_partial.hidden_states[-1],\n",
    "      #output_full.hidden_states[-1]\n",
    "      torch.cat(output_partial.hidden_states, 0),\n",
    "      torch.cat(output_full.hidden_states, 0)\n",
    "    )\n",
    "\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  return loss\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_train.parameters(), lr=1e-6)\n",
    "\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.add_text(\"name\", f\"{MODEL} v3\")\n",
    "\n",
    "num_length = 0\n",
    "step = 0\n",
    "for epoch in range(8):\n",
    "  pbar = tqdm(ds.shuffle(seed=epoch))\n",
    "  for example in pbar:\n",
    "    example = get_sample(example)\n",
    "    if example[\"length\"] > 29.0: continue\n",
    "\n",
    "    loss = compute_hidden_state_loss(model_train, model_base, optimizer, criterion, example)\n",
    "    step += 1\n",
    "    num_length += example[\"length\"]\n",
    "\n",
    "    writer.add_scalar(\"loss/train\", loss.item(), step)\n",
    "    writer.add_scalar(\"length/train\", num_length, step)\n",
    "    writer.add_scalar(\"epoch/train\", epoch, step)\n",
    "\n",
    "    pbar.set_description(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxnuwcBmMSjh",
    "outputId": "0fc0ffd0-8c3f-4d59-88ba-e224a92c27f7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Select an audio file and read it:\n",
    "ds_eval = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# Load the Whisper model in Hugging Face format:\n",
    "model = WhisperForConditionalGeneration.from_pretrained(f\"openai/whisper-{MODEL}\").eval().cuda()\n",
    "\n",
    "for i in range(64):\n",
    "  audio_sample = ds_eval[i][\"audio\"]\n",
    "  waveform = audio_sample[\"array\"]\n",
    "  sampling_rate = audio_sample[\"sampling_rate\"]\n",
    "\n",
    "  # Use the model and processor to transcribe the audio:\n",
    "  input_features = processor(\n",
    "      waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\n",
    "  ).input_features.cuda()\n",
    "\n",
    "  model.model = model_base.eval().cuda()\n",
    "  predicted_ids_base = model.generate(input_features)\n",
    "  model.model = model_train.eval().cuda()\n",
    "  predicted_ids_train = model.generate(input_features)\n",
    "\n",
    "  # Decode token ids to text\n",
    "  transcription = processor.batch_decode([predicted_ids_base[0], predicted_ids_train[0]], skip_special_tokens=True)\n",
    "\n",
    "  print(f\"\\n\\nGrndTr: {ds_eval[i]['text'].lower()}\\nModelB:{transcription[0]}\\nModelT:{transcription[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "9if_LdqbOVNI",
    "outputId": "b61f3e85-d3c3-4a6b-dc26-1950301e24fa"
   },
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(f\"openai/whisper-{MODEL}\").eval().cpu()\n",
    "model.model = model_train.eval().cpu()\n",
    "\n",
    "model.save_pretrained(f\"model_train-{MODEL}3\")\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive(f\"model_train-{MODEL}3\", 'zip', f\"model_train-{MODEL}3\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
